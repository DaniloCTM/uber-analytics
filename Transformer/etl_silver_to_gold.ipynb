{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed63491",
   "metadata": {},
   "source": [
    "# ETL: Silver → Gold Layer\n",
    "\n",
    "**Objetivo:** Transformar dados do Silver em Data Warehouse (Gold) com Star Schema.\n",
    "\n",
    "**Processo:**\n",
    "1. Extrair dados de `silver.uber_silver`\n",
    "2. Popular 4 dimensões consolidadas (dim_dtt, dim_cst, dim_loc, dim_rid)\n",
    "3. Popular tabela fato `fat_cor` com métricas e FKs\n",
    "4. Validar Data Warehouse\n",
    "\n",
    "**Schema:** dwh.dim_dtt, dim_cst, dim_loc, dim_rid → fat_cor\n",
    "\n",
    "**Mnemônicos:** Utiliza nomenclatura silábica (3-4 caracteres). Veja docs/Mnemonicos_DWH.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9d0c6269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from psycopg2.extras import execute_values\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a031ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Conexão estabelecida!\n"
     ]
    }
   ],
   "source": [
    "def get_connection():\n    load_dotenv()\n    \n    DB_USER = os.getenv('POSTGRES_USER', 'postgres')\n    DB_PASSWORD = os.getenv('POSTGRES_PASSWORD', 'postgres')\n    DB_HOST = os.getenv('POSTGRES_HOST', 'localhost')\n    DB_PORT = os.getenv('POSTGRES_PORT', '5432')\n    DB_NAME = os.getenv('POSTGRES_DB', 'uber')  # Corrigido: POSTGRES_DB\n\n    return psycopg2.connect(\n        host=DB_HOST,\n        dbname=DB_NAME,\n        user=DB_USER,\n        password=DB_PASSWORD,\n        port=DB_PORT\n    )\n\n# Teste conexão\ntry:\n    conn = get_connection()\n    print(\" Conexão estabelecida!\")\n    conn.close()\nexcept Exception as e:\n    print(f\" Erro: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a770d",
   "metadata": {},
   "source": [
    "## 0. PREPARAÇÃO: Criar Schema e Tabelas do DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c51767e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lendo arquivo gold_ddl.sql...\n",
      " Executando DDL no PostgreSQL...\n",
      " Schema 'dwh' e todas as tabelas criadas com sucesso!\n",
      "\n",
      " Tabelas criadas no schema 'dwh': 5\n",
      "   • dim_cst\n",
      "   • dim_dtt\n",
      "   • dim_loc\n",
      "   • dim_rid\n",
      "   • fat_cor\n"
     ]
    }
   ],
   "source": [
    "# Executar DDL do Gold para criar schema e tabelas\nimport os\n\nddl_path = os.path.join('..', 'Data Layer', 'gold', 'gold_ddl.sql')\n\nprint(\" Lendo arquivo gold_ddl.sql...\")\nwith open(ddl_path, 'r', encoding='utf-8') as f:\n    ddl_script = f.read()\n\nprint(\" Executando DDL no PostgreSQL...\")\nconn = get_connection()\ncur = conn.cursor()\n\ntry:\n    cur.execute(ddl_script)\n    conn.commit()\n    print(\" Schema 'dwh' e todas as tabelas criadas com sucesso!\")\n    \n    # Verificar tabelas criadas\n    cur.execute(\"\"\"\n        SELECT table_name \n        FROM information_schema.tables \n        WHERE table_schema = 'dwh'\n        ORDER BY table_name;\n    \"\"\")\n    tabelas = cur.fetchall()\n    print(f\"\\n Tabelas criadas no schema 'dwh': {len(tabelas)}\")\n    for tabela in tabelas:\n        print(f\"   • {tabela[0]}\")\n        \nexcept Exception as e:\n    conn.rollback()\n    print(f\" Erro ao executar DDL: {e}\")\n    raise\nfinally:\n    cur.close()\n    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399ed4e5",
   "metadata": {},
   "source": [
    "## 1. EXTRAÇÃO: Carregar Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a95c30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61865/372627024.py:11: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_silver = pd.read_sql(query_silver, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Registros carregados: 148,767\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>booking_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>vehicle_type</th>\n",
       "      <th>pickup_location</th>\n",
       "      <th>drop_location</th>\n",
       "      <th>booking_value</th>\n",
       "      <th>ride_distance</th>\n",
       "      <th>payment_method</th>\n",
       "      <th>booking_status</th>\n",
       "      <th>reason_for_cancelling_by_customer</th>\n",
       "      <th>driver_cancellation_reason</th>\n",
       "      <th>incomplete_rides_reason</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>avg_vtat</th>\n",
       "      <th>avg_ctat</th>\n",
       "      <th>driver_ratings</th>\n",
       "      <th>customer_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNR4352144</td>\n",
       "      <td>CID8362794</td>\n",
       "      <td>Bike</td>\n",
       "      <td>Udyog Vihar</td>\n",
       "      <td>Ambience Mall</td>\n",
       "      <td>99.00000</td>\n",
       "      <td>37.980000</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>00:19:34</td>\n",
       "      <td>10.8</td>\n",
       "      <td>38.900000</td>\n",
       "      <td>4.800000</td>\n",
       "      <td>4.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNR9147645</td>\n",
       "      <td>CID8300238</td>\n",
       "      <td>Go Mini</td>\n",
       "      <td>Basai Dhankot</td>\n",
       "      <td>Madipur</td>\n",
       "      <td>114.00000</td>\n",
       "      <td>39.290000</td>\n",
       "      <td>Uber Wallet</td>\n",
       "      <td>Completed</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>01:35:18</td>\n",
       "      <td>8.5</td>\n",
       "      <td>15.100000</td>\n",
       "      <td>4.200000</td>\n",
       "      <td>4.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNR1009222</td>\n",
       "      <td>CID2030746</td>\n",
       "      <td>Go Sedan</td>\n",
       "      <td>Tughlakabad</td>\n",
       "      <td>Greater Kailash</td>\n",
       "      <td>508.29023</td>\n",
       "      <td>24.640956</td>\n",
       "      <td>UPI</td>\n",
       "      <td>Cancelled by Driver</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>More than permitted people in there</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>01:37:50</td>\n",
       "      <td>7.4</td>\n",
       "      <td>29.150249</td>\n",
       "      <td>4.230756</td>\n",
       "      <td>4.404301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNR2740479</td>\n",
       "      <td>CID3231181</td>\n",
       "      <td>Auto</td>\n",
       "      <td>Palam Vihar</td>\n",
       "      <td>Kherki Daula Toll</td>\n",
       "      <td>508.29023</td>\n",
       "      <td>24.640956</td>\n",
       "      <td>UPI</td>\n",
       "      <td>Cancelled by Driver</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>Personal &amp; Car related issues</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>01:48:03</td>\n",
       "      <td>5.6</td>\n",
       "      <td>29.150249</td>\n",
       "      <td>4.230756</td>\n",
       "      <td>4.404301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNR7650148</td>\n",
       "      <td>CID3381661</td>\n",
       "      <td>Go Sedan</td>\n",
       "      <td>Narsinghpur</td>\n",
       "      <td>Pulbangash</td>\n",
       "      <td>508.29023</td>\n",
       "      <td>24.640956</td>\n",
       "      <td>UPI</td>\n",
       "      <td>Cancelled by Driver</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>More than permitted people in there</td>\n",
       "      <td>Reason Unknown</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>01:49:56</td>\n",
       "      <td>6.2</td>\n",
       "      <td>29.150249</td>\n",
       "      <td>4.230756</td>\n",
       "      <td>4.404301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   booking_id customer_id vehicle_type pickup_location      drop_location  \\\n",
       "0  CNR4352144  CID8362794         Bike     Udyog Vihar      Ambience Mall   \n",
       "1  CNR9147645  CID8300238      Go Mini   Basai Dhankot            Madipur   \n",
       "2  CNR1009222  CID2030746     Go Sedan     Tughlakabad    Greater Kailash   \n",
       "3  CNR2740479  CID3231181         Auto     Palam Vihar  Kherki Daula Toll   \n",
       "4  CNR7650148  CID3381661     Go Sedan     Narsinghpur         Pulbangash   \n",
       "\n",
       "   booking_value  ride_distance payment_method       booking_status  \\\n",
       "0       99.00000      37.980000           Cash            Completed   \n",
       "1      114.00000      39.290000    Uber Wallet            Completed   \n",
       "2      508.29023      24.640956            UPI  Cancelled by Driver   \n",
       "3      508.29023      24.640956            UPI  Cancelled by Driver   \n",
       "4      508.29023      24.640956            UPI  Cancelled by Driver   \n",
       "\n",
       "  reason_for_cancelling_by_customer           driver_cancellation_reason  \\\n",
       "0                    Reason Unknown                       Reason Unknown   \n",
       "1                    Reason Unknown                       Reason Unknown   \n",
       "2                    Reason Unknown  More than permitted people in there   \n",
       "3                    Reason Unknown        Personal & Car related issues   \n",
       "4                    Reason Unknown  More than permitted people in there   \n",
       "\n",
       "  incomplete_rides_reason       date      time  avg_vtat   avg_ctat  \\\n",
       "0          Reason Unknown 2024-01-01  00:19:34      10.8  38.900000   \n",
       "1          Reason Unknown 2024-01-01  01:35:18       8.5  15.100000   \n",
       "2          Reason Unknown 2024-01-01  01:37:50       7.4  29.150249   \n",
       "3          Reason Unknown 2024-01-01  01:48:03       5.6  29.150249   \n",
       "4          Reason Unknown 2024-01-01  01:49:56       6.2  29.150249   \n",
       "\n",
       "   driver_ratings  customer_rating  \n",
       "0        4.800000         4.800000  \n",
       "1        4.200000         4.100000  \n",
       "2        4.230756         4.404301  \n",
       "3        4.230756         4.404301  \n",
       "4        4.230756         4.404301  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_silver = \"\"\"\nSELECT booking_id, customer_id, vehicle_type, pickup_location, drop_location,\n       booking_value, ride_distance, payment_method, booking_status,\n       reason_for_cancelling_by_customer, driver_cancellation_reason, incomplete_rides_reason,\n       date, time, avg_vtat, avg_ctat, driver_ratings, customer_rating\nFROM silver.uber_silver\nORDER BY date, time;\n\"\"\"\n\nconn = get_connection()\ndf_silver = pd.read_sql(query_silver, conn)\nconn.close()\n\nprint(f\" Registros carregados: {len(df_silver):,}\")\ndf_silver.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbb12ad",
   "metadata": {},
   "source": [
    "## 2. TRANSFORMAÇÃO: Criar Dimensões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "95857a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_dtt: 124,452 registros (2024-01-01 a 2024-12-30)\n",
      "   Exemplo trimestre: Q1=Jan-Mar, Q2=Abr-Jun, Q3=Jul-Set, Q4=Out-Dez\n",
      "   Verificação: 124452 chaves únicas de 124452 registros\n"
     ]
    }
   ],
   "source": [
    "# 2.1 dim_dtt (DateTime - Data e Hora combinadas)\ndf_silver['date'] = pd.to_datetime(df_silver['date'])\ndf_silver['datetime'] = pd.to_datetime(df_silver['date'].astype(str) + ' ' + df_silver['time'].astype(str))\n\n# Criar dtt_key único: YYYYMMDDHHMM\ndf_silver['dtt_key'] = df_silver['datetime'].dt.strftime('%Y%m%d%H%M').astype('Int64')\n\n# Garantir que não há duplicatas no dtt_key (pegar apenas o primeiro de cada grupo)\ndim_dtt = df_silver[['dtt_key', 'datetime']].drop_duplicates(subset=['dtt_key']).copy()\ndim_dtt['dat'] = dim_dtt['datetime'].dt.date\ndim_dtt['hor'] = dim_dtt['datetime'].dt.time\ndim_dtt['yrr'] = dim_dtt['datetime'].dt.year\ndim_dtt['qtr'] = dim_dtt['datetime'].dt.quarter  # Trimestre 1-4\ndim_dtt['mth'] = dim_dtt['datetime'].dt.month\ndim_dtt['day'] = dim_dtt['datetime'].dt.day\ndim_dtt['dow'] = dim_dtt['datetime'].dt.dayofweek + 1  # 1=Segunda, 7=Domingo\ndim_dtt['wkd'] = dim_dtt['datetime'].dt.dayofweek.isin([5, 6]).map({True: 'Yes', False: 'No'})\n\ndim_dtt = dim_dtt.sort_values('dtt_key')\nprint(f\" dim_dtt: {len(dim_dtt):,} registros ({dim_dtt['dat'].min()} a {dim_dtt['dat'].max()})\")\nprint(f\"   Exemplo trimestre: Q1=Jan-Mar, Q2=Abr-Jun, Q3=Jul-Set, Q4=Out-Dez\")\nprint(f\"   Verificação: {dim_dtt['dtt_key'].nunique()} chaves únicas de {len(dim_dtt)} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "79a010d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_cst: 147,580 registros\n"
     ]
    }
   ],
   "source": [
    "# 2.2 dim_cst (Customer - Cliente)\ndim_cst = df_silver[['customer_id']].drop_duplicates().copy()\ndim_cst = dim_cst.rename(columns={'customer_id': 'cst_ide'})\ndim_cst['dat_cad'] = df_silver.groupby('customer_id')['date'].min().values\n\nprint(f\" dim_cst: {len(dim_cst):,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eeeaba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_loc: 176 registros\n"
     ]
    }
   ],
   "source": [
    "# 2.3 dim_loc (Location - Localização role-playing)\npickup_loc = df_silver[['pickup_location']].rename(columns={'pickup_location': 'loc_nme'})\ndrop_loc = df_silver[['drop_location']].rename(columns={'drop_location': 'loc_nme'})\ndim_loc = pd.concat([pickup_loc, drop_loc]).drop_duplicates()\ndim_loc['rgn'] = None  # Região (a ser derivada via geocoding)\ndim_loc['zon'] = None  # Zona (a ser derivada)\n\nprint(f\" dim_loc: {len(dim_loc):,} registros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44145fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_rid: 102,023 registros\n"
     ]
    }
   ],
   "source": [
    "# 2.4 dim_rid (Ride - Atributos consolidados da corrida)\n# Consolidar atributos únicos: vehicle_type, booking_status, payment_method, motivos, tempos\ndim_rid = df_silver[[\n    'vehicle_type', 'booking_status', 'payment_method',\n    'reason_for_cancelling_by_customer', 'driver_cancellation_reason', 'incomplete_rides_reason',\n    'avg_vtat', 'avg_ctat'\n]].drop_duplicates().copy()\n\ndim_rid = dim_rid.rename(columns={\n    'vehicle_type': 'vhc_tpe',\n    'booking_status': 'bkg_sts',\n    'payment_method': 'pmt_mtd',\n    'reason_for_cancelling_by_customer': 'rsn_cst',\n    'driver_cancellation_reason': 'rsn_drv',\n    'incomplete_rides_reason': 'rsn_inc',\n    'avg_vtat': 'avg_vtt',\n    'avg_ctat': 'avg_ctt'\n})\n\nprint(f\" dim_rid: {len(dim_rid):,} registros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdda0f5",
   "metadata": {},
   "source": [
    "## 3. CARGA: Inserir Dimensões no DWH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fb124806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tabela dim_dtt truncada\n",
      " dim_dtt inserida: 124,452 registros\n",
      " dim_dtt inserida: 124,452 registros\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Inserir dim_dtt (DateTime)\nconn = get_connection()\ncur = conn.cursor()\n\ntry:\n    cur.execute(\"TRUNCATE TABLE dwh.dim_dtt CASCADE;\")\n    conn.commit()\n    print(\" Tabela dim_dtt truncada\")\nexcept Exception as e:\n    print(f\" Aviso no TRUNCATE: {e}\")\n    conn.rollback()\n    # Se TRUNCATE falhar, usar DELETE\n    cur.execute(\"DELETE FROM dwh.dim_dtt;\")\n    conn.commit()\n    print(\" Tabela dim_dtt limpa com DELETE\")\n\n# Verificar duplicatas antes de inserir\nif dim_dtt['dtt_key'].duplicated().any():\n    print(f\" ATENÇÃO: Encontradas {dim_dtt['dtt_key'].duplicated().sum()} duplicatas em dtt_key!\")\n    dim_dtt = dim_dtt.drop_duplicates(subset=['dtt_key'], keep='first')\n    print(f\"   Removidas duplicatas. Registros finais: {len(dim_dtt)}\")\n\ndtt_values = [\n    (int(row['dtt_key']), row['dat'], row['hor'], int(row['yrr']), int(row['qtr']),\n     int(row['mth']), int(row['day']), int(row['dow']), row['wkd'])\n    for _, row in dim_dtt.iterrows()\n]\n\ninsert_query = \"\"\"\nINSERT INTO dwh.dim_dtt (dtt_key, dat, hor, yrr, qtr, mth, day, dow, wkd)\nVALUES %s\n\"\"\"\nexecute_values(cur, insert_query, dtt_values, page_size=1000)\nconn.commit()\n\ncur.execute(\"SELECT COUNT(*) FROM dwh.dim_dtt;\")\nprint(f\" dim_dtt inserida: {cur.fetchone()[0]:,} registros\")\ncur.close()\nconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "233d4168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_cst inserida: 147,580 registros\n"
     ]
    }
   ],
   "source": [
    "# 3.2 Inserir dim_cst (Customer)\nconn = get_connection()\ncur = conn.cursor()\n\ntry:\n    cur.execute(\"TRUNCATE TABLE dwh.dim_cst CASCADE;\")\n    conn.commit()\nexcept Exception as e:\n    print(f\" Aviso no TRUNCATE: {e}\")\n    conn.rollback()\n    cur.execute(\"DELETE FROM dwh.dim_cst;\")\n    conn.commit()\n\nfor _, row in dim_cst.iterrows():\n    cur.execute(\"\"\"\n    INSERT INTO dwh.dim_cst (cst_ide, dat_cad)\n    VALUES (%s, %s)\n    ON CONFLICT (cst_ide) DO UPDATE SET\n        dat_cad = EXCLUDED.dat_cad;\n    \"\"\", (row['cst_ide'], row['dat_cad'].date()))\n\nconn.commit()\ncur.execute(\"SELECT COUNT(*) FROM dwh.dim_cst;\")\nprint(f\" dim_cst inserida: {cur.fetchone()[0]:,} registros\")\ncur.close()\nconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4ef3c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_loc inserida: 176 registros\n"
     ]
    }
   ],
   "source": [
    "# 3.3 Inserir dim_loc (Location)\nconn = get_connection()\ncur = conn.cursor()\n\ntry:\n    cur.execute(\"TRUNCATE TABLE dwh.dim_loc CASCADE;\")\n    conn.commit()\nexcept Exception as e:\n    print(f\" Aviso no TRUNCATE: {e}\")\n    conn.rollback()\n    cur.execute(\"DELETE FROM dwh.dim_loc;\")\n    conn.commit()\n\nfor _, row in dim_loc.iterrows():\n    cur.execute(\"\"\"\n    INSERT INTO dwh.dim_loc (loc_nme, rgn, zon)\n    VALUES (%s, %s, %s)\n    ON CONFLICT (loc_nme) DO UPDATE SET\n        rgn = EXCLUDED.rgn,\n        zon = EXCLUDED.zon;\n    \"\"\", (row['loc_nme'], row['rgn'], row['zon']))\n\nconn.commit()\ncur.execute(\"SELECT COUNT(*) FROM dwh.dim_loc;\")\nprint(f\" dim_loc inserida: {cur.fetchone()[0]:,} registros\")\ncur.close()\nconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af080630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " dim_rid inserida: 102,023 registros\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Inserir dim_rid (Ride)\nconn = get_connection()\ncur = conn.cursor()\n\ntry:\n    cur.execute(\"TRUNCATE TABLE dwh.dim_rid CASCADE;\")\n    conn.commit()\nexcept Exception as e:\n    print(f\" Aviso no TRUNCATE: {e}\")\n    conn.rollback()\n    cur.execute(\"DELETE FROM dwh.dim_rid;\")\n    conn.commit()\n\n# Usar execute_values para melhor performance\nrid_values = [\n    (row['vhc_tpe'], row['bkg_sts'], row['pmt_mtd'], row['rsn_cst'], \n     row['rsn_drv'], row['rsn_inc'], row['avg_vtt'], row['avg_ctt'])\n    for _, row in dim_rid.iterrows()\n]\n\ninsert_query = \"\"\"\nINSERT INTO dwh.dim_rid (vhc_tpe, bkg_sts, pmt_mtd, rsn_cst, rsn_drv, rsn_inc, avg_vtt, avg_ctt)\nVALUES %s\n\"\"\"\nexecute_values(cur, insert_query, rid_values, page_size=1000)\nconn.commit()\n\ncur.execute(\"SELECT COUNT(*) FROM dwh.dim_rid;\")\nprint(f\" dim_rid inserida: {cur.fetchone()[0]:,} registros\")\ncur.close()\nconn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f490fe",
   "metadata": {},
   "source": [
    "## 4. PREPARAR FATO: Lookups e Transformações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abd2ce6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61865/1774960915.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  lookup_cst = pd.read_sql(\"SELECT srk_cst, cst_ide FROM dwh.dim_cst\", conn)\n",
      "/tmp/ipykernel_61865/1774960915.py:4: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  lookup_loc = pd.read_sql(\"SELECT srk_loc, loc_nme FROM dwh.dim_loc\", conn)\n",
      "/tmp/ipykernel_61865/1774960915.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  lookup_rid = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lookups carregados\n",
      "   Clientes: 147,580\n",
      "   Localizações: 176\n",
      "   Ride Attributes: 102,023\n"
     ]
    }
   ],
   "source": [
    "# Carregar lookups das dimensões\nconn = get_connection()\nlookup_cst = pd.read_sql(\"SELECT srk_cst, cst_ide FROM dwh.dim_cst\", conn)\nlookup_loc = pd.read_sql(\"SELECT srk_loc, loc_nme FROM dwh.dim_loc\", conn)\nlookup_rid = pd.read_sql(\"\"\"\n    SELECT srk_rid, vhc_tpe, bkg_sts, pmt_mtd, rsn_cst, rsn_drv, rsn_inc, avg_vtt, avg_ctt \n    FROM dwh.dim_rid\n\"\"\", conn)\nconn.close()\n\nprint(f\" Lookups carregados\")\nprint(f\"   Clientes: {len(lookup_cst):,}\")\nprint(f\"   Localizações: {len(lookup_loc):,}\")\nprint(f\"   Ride Attributes: {len(lookup_rid):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66b49872",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61865/3305552655.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  lookup_dtt = pd.read_sql(\"SELECT srk_dtt, dtt_key FROM dwh.dim_dtt\", conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Lookup dim_dtt: 124,452 registros\n",
      " Fato preparada: 148,767 registros\n",
      "   Completas: 101,175\n",
      "   Canceladas: 37,191\n",
      "   Incompletas: 8,927\n",
      " Fato preparada: 148,767 registros\n",
      "   Completas: 101,175\n",
      "   Canceladas: 37,191\n",
      "   Incompletas: 8,927\n"
     ]
    }
   ],
   "source": [
    "# Preparar tabela fato\ndf_fato = df_silver.copy()\n\n# Criar dtt_key (já criado em dim_dtt)\ndf_fato['datetime'] = pd.to_datetime(df_fato['date'].astype(str) + ' ' + df_fato['time'].astype(str))\ndf_fato['dtt_key'] = df_fato['datetime'].dt.strftime('%Y%m%d%H%M').astype('Int64')\n\n# Merge com dim_dtt para obter srk_dtt\nconn = get_connection()\nlookup_dtt = pd.read_sql(\"SELECT srk_dtt, dtt_key FROM dwh.dim_dtt\", conn)\nconn.close()\ndf_fato = df_fato.merge(lookup_dtt, on='dtt_key', how='left')\nprint(f\" Lookup dim_dtt: {len(lookup_dtt):,} registros\")\n\n# Merge com dim_cst (customer)\ndf_fato = df_fato.merge(\n    lookup_cst.rename(columns={'cst_ide': 'customer_id'}), \n    on='customer_id', how='left'\n)\n\n# Merge com dim_rid (ride attributes)\ndf_fato = df_fato.merge(\n    lookup_rid.rename(columns={\n        'vhc_tpe': 'vehicle_type',\n        'bkg_sts': 'booking_status',\n        'pmt_mtd': 'payment_method',\n        'rsn_cst': 'reason_for_cancelling_by_customer',\n        'rsn_drv': 'driver_cancellation_reason',\n        'rsn_inc': 'incomplete_rides_reason',\n        'avg_vtt': 'avg_vtat',\n        'avg_ctt': 'avg_ctat'\n    }),\n    on=['vehicle_type', 'booking_status', 'payment_method', \n        'reason_for_cancelling_by_customer', 'driver_cancellation_reason', \n        'incomplete_rides_reason', 'avg_vtat', 'avg_ctat'],\n    how='left'\n)\n\n# Merge com dim_loc para pickup\ndf_fato = df_fato.merge(\n    lookup_loc.rename(columns={'srk_loc': 'srk_pck', 'loc_nme': 'pickup_location'}),\n    on='pickup_location', how='left'\n)\n\n# Merge com dim_loc para drop (role-playing)\ndf_fato = df_fato.merge(\n    lookup_loc.rename(columns={'srk_loc': 'srk_drp', 'loc_nme': 'drop_location'}),\n    on='drop_location', how='left'\n)\n\n# Calcular métricas derivadas\ndf_fato['amt_km'] = df_fato.apply(\n    lambda x: round(x['booking_value'] / x['ride_distance'], 2) \n    if pd.notna(x['ride_distance']) and x['ride_distance'] > 0 else None,\n    axis=1\n)\n\n# Flags VARCHAR(3) com 'Yes'/'No'\ndf_fato['flg_cmp'] = df_fato['booking_status'].str.lower().str.contains('complete', na=False).map({True: 'Yes', False: 'No'})\ndf_fato['flg_cnc'] = df_fato['booking_status'].str.lower().str.contains('cancel', na=False).map({True: 'Yes', False: 'No'})\ndf_fato['flg_inc'] = df_fato['booking_status'].str.lower().str.contains('incomplete', na=False).map({True: 'Yes', False: 'No'})\n\nprint(f\" Fato preparada: {len(df_fato):,} registros\")\nprint(f\"   Completas: {(df_fato['flg_cmp'] == 'Yes').sum():,}\")\nprint(f\"   Canceladas: {(df_fato['flg_cnc'] == 'Yes').sum():,}\")\nprint(f\"   Incompletas: {(df_fato['flg_inc'] == 'Yes').sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79188bc",
   "metadata": {},
   "source": [
    "## 5. CARGA: Inserir Tabela Fato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e3bdbb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Inserindo 148,767 registros em 149 batches...\n",
      "   Batch 10/149\n",
      "   Batch 10/149\n",
      "   Batch 20/149\n",
      "   Batch 20/149\n",
      "   Batch 30/149\n",
      "   Batch 30/149\n",
      "   Batch 40/149\n",
      "   Batch 40/149\n",
      "   Batch 50/149\n",
      "   Batch 50/149\n",
      "   Batch 60/149\n",
      "   Batch 60/149\n",
      "   Batch 70/149\n",
      "   Batch 70/149\n",
      "   Batch 80/149\n",
      "   Batch 80/149\n",
      "   Batch 90/149\n",
      "   Batch 90/149\n",
      "   Batch 100/149\n",
      "   Batch 100/149\n",
      "   Batch 110/149\n",
      "   Batch 110/149\n",
      "   Batch 120/149\n",
      "   Batch 120/149\n",
      "   Batch 130/149\n",
      "   Batch 130/149\n",
      "   Batch 140/149\n",
      "   Batch 140/149\n",
      "\n",
      " fat_cor inserida: 148,767 registros\n",
      "\n",
      " fat_cor inserida: 148,767 registros\n"
     ]
    }
   ],
   "source": [
    "# Selecionar colunas para inserção na fat_cor\nfato_columns = [\n    'booking_id',  # cor_key\n    'srk_dtt', 'srk_cst', 'srk_rid', 'srk_pck', 'srk_drp',\n    'booking_value', 'ride_distance', 'driver_ratings', 'customer_rating', 'amt_km',\n    'flg_cmp', 'flg_cnc', 'flg_inc'\n]\n\ndf_fato_insert = df_fato[fato_columns].where(pd.notnull(df_fato[fato_columns]), None)\nfato_values = [tuple(row) for row in df_fato_insert.values]\n\nconn = get_connection()\ncur = conn.cursor()\ncur.execute(\"TRUNCATE TABLE dwh.fat_cor;\")\n\ninsert_query = \"\"\"\nINSERT INTO dwh.fat_cor (\n    cor_key, srk_dtt, srk_cst, srk_rid, srk_pck, srk_drp,\n    amt, dst, rtg_drv, rtg_cst, amt_km,\n    flg_cmp, flg_cnc, flg_inc\n)\nVALUES %s\nON CONFLICT (cor_key) DO NOTHING;\n\"\"\"\n\n# Inserir em batches\nbatch_size = 1000\ntotal_batches = (len(fato_values) + batch_size - 1) // batch_size\nprint(f\" Inserindo {len(fato_values):,} registros em {total_batches} batches...\")\n\nfor i in range(0, len(fato_values), batch_size):\n    batch = fato_values[i:i+batch_size]\n    execute_values(cur, insert_query, batch, page_size=batch_size)\n    if (i // batch_size + 1) % 10 == 0:\n        print(f\"   Batch {i // batch_size + 1}/{total_batches}\")\n\nconn.commit()\ncur.execute(\"SELECT COUNT(*) FROM dwh.fat_cor;\")\nprint(f\"\\n fat_cor inserida: {cur.fetchone()[0]:,} registros\")\ncur.close()\nconn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0403cd66",
   "metadata": {},
   "source": [
    "## 6. VALIDAÇÃO do Data Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "82ac36bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      " VALIDAÇÃO DO DATA WAREHOUSE\n",
      "======================================================================\n",
      "Total Corridas....................................         148,767\n",
      "Corridas Completas................................         101,175\n",
      "Corridas Canceladas...............................          37,191\n",
      "Corridas Incompletas..............................           8,927\n",
      "Total Clientes....................................         147,580\n",
      "Total Localizações................................             176\n",
      "Total DateTime Registros..........................         124,452\n",
      "Total Ride Attributes.............................         102,023\n",
      "Receita Total (R$)................................   75,616,801.68\n",
      "Distância Total (km)..............................    3,665,761.05\n",
      "Média Rating Motorista............................            4.23\n",
      "Média Rating Cliente..............................            4.40\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61865/1863132824.py:22: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  result = pd.read_sql(query, conn).iloc[0, 0]\n"
     ]
    }
   ],
   "source": [
    "# Verificar integridade do DWH\nconn = get_connection()\nvalidation_queries = {\n    'Total Corridas': \"SELECT COUNT(*) FROM dwh.fat_cor\",\n    'Corridas Completas': \"SELECT COUNT(*) FROM dwh.fat_cor WHERE flg_cmp = 'Yes'\",\n    'Corridas Canceladas': \"SELECT COUNT(*) FROM dwh.fat_cor WHERE flg_cnc = 'Yes'\",\n    'Corridas Incompletas': \"SELECT COUNT(*) FROM dwh.fat_cor WHERE flg_inc = 'Yes'\",\n    'Total Clientes': \"SELECT COUNT(*) FROM dwh.dim_cst\",\n    'Total Localizações': \"SELECT COUNT(*) FROM dwh.dim_loc\",\n    'Total DateTime Registros': \"SELECT COUNT(*) FROM dwh.dim_dtt\",\n    'Total Ride Attributes': \"SELECT COUNT(*) FROM dwh.dim_rid\",\n    'Receita Total (R$)': \"SELECT SUM(amt) FROM dwh.fat_cor\",\n    'Distância Total (km)': \"SELECT SUM(dst) FROM dwh.fat_cor\",\n    'Média Rating Motorista': \"SELECT AVG(rtg_drv) FROM dwh.fat_cor WHERE rtg_drv IS NOT NULL\",\n    'Média Rating Cliente': \"SELECT AVG(rtg_cst) FROM dwh.fat_cor WHERE rtg_cst IS NOT NULL\"\n}\n\nprint(\"=\"*70)\nprint(\" VALIDAÇÃO DO DATA WAREHOUSE\")\nprint(\"=\"*70)\nfor label, query in validation_queries.items():\n    result = pd.read_sql(query, conn).iloc[0, 0]\n    if isinstance(result, (int, np.integer)):\n        print(f\"{label:.<50} {result:>15,}\")\n    elif isinstance(result, (float, np.floating)):\n        print(f\"{label:.<50} {result:>15,.2f}\")\nprint(\"=\"*70)\nconn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "73625f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_61865/745244107.py:3: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_top_rotas = pd.read_sql(\"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " TOP 10 ROTAS MAIS RENTÁVEIS:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>origem</th>\n",
       "      <th>destino</th>\n",
       "      <th>total_corridas</th>\n",
       "      <th>receita_total</th>\n",
       "      <th>ticket_medio</th>\n",
       "      <th>distancia_media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>New Delhi Railway Station</td>\n",
       "      <td>Rajouri Garden</td>\n",
       "      <td>6</td>\n",
       "      <td>9559.0</td>\n",
       "      <td>1593.166667</td>\n",
       "      <td>23.798333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cyber Hub</td>\n",
       "      <td>Gurgaon Railway Station</td>\n",
       "      <td>10</td>\n",
       "      <td>9348.0</td>\n",
       "      <td>934.800000</td>\n",
       "      <td>32.522000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nirman Vihar</td>\n",
       "      <td>Vatika Chowk</td>\n",
       "      <td>5</td>\n",
       "      <td>9284.0</td>\n",
       "      <td>1856.800000</td>\n",
       "      <td>29.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashok Vihar</td>\n",
       "      <td>Basai Dhankot</td>\n",
       "      <td>9</td>\n",
       "      <td>9280.0</td>\n",
       "      <td>1031.111111</td>\n",
       "      <td>24.782222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Anand Vihar ISBT</td>\n",
       "      <td>Noida Film City</td>\n",
       "      <td>7</td>\n",
       "      <td>8960.0</td>\n",
       "      <td>1280.000000</td>\n",
       "      <td>24.452857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mayur Vihar</td>\n",
       "      <td>Samaypur Badli</td>\n",
       "      <td>9</td>\n",
       "      <td>8588.0</td>\n",
       "      <td>954.222222</td>\n",
       "      <td>17.853333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Model Town</td>\n",
       "      <td>Jahangirpuri</td>\n",
       "      <td>8</td>\n",
       "      <td>8540.0</td>\n",
       "      <td>1067.500000</td>\n",
       "      <td>26.116250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ambience Mall</td>\n",
       "      <td>Akshardham</td>\n",
       "      <td>11</td>\n",
       "      <td>8518.0</td>\n",
       "      <td>774.363636</td>\n",
       "      <td>26.936364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Greater Noida</td>\n",
       "      <td>Jor Bagh</td>\n",
       "      <td>8</td>\n",
       "      <td>8252.0</td>\n",
       "      <td>1031.500000</td>\n",
       "      <td>32.488750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Greater Noida</td>\n",
       "      <td>Rithala</td>\n",
       "      <td>7</td>\n",
       "      <td>8082.0</td>\n",
       "      <td>1154.571429</td>\n",
       "      <td>30.317143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      origem                  destino  total_corridas  \\\n",
       "0  New Delhi Railway Station           Rajouri Garden               6   \n",
       "1                  Cyber Hub  Gurgaon Railway Station              10   \n",
       "2               Nirman Vihar             Vatika Chowk               5   \n",
       "3                Ashok Vihar            Basai Dhankot               9   \n",
       "4           Anand Vihar ISBT          Noida Film City               7   \n",
       "5                Mayur Vihar           Samaypur Badli               9   \n",
       "6                 Model Town             Jahangirpuri               8   \n",
       "7              Ambience Mall               Akshardham              11   \n",
       "8              Greater Noida                 Jor Bagh               8   \n",
       "9              Greater Noida                  Rithala               7   \n",
       "\n",
       "   receita_total  ticket_medio  distancia_media  \n",
       "0         9559.0   1593.166667        23.798333  \n",
       "1         9348.0    934.800000        32.522000  \n",
       "2         9284.0   1856.800000        29.890000  \n",
       "3         9280.0   1031.111111        24.782222  \n",
       "4         8960.0   1280.000000        24.452857  \n",
       "5         8588.0    954.222222        17.853333  \n",
       "6         8540.0   1067.500000        26.116250  \n",
       "7         8518.0    774.363636        26.936364  \n",
       "8         8252.0   1031.500000        32.488750  \n",
       "9         8082.0   1154.571429        30.317143  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query analítica: Top 10 rotas por receita\nconn = get_connection()\ndf_top_rotas = pd.read_sql(\"\"\"\nSELECT \n    pickup.loc_nme AS origem, \n    drop.loc_nme AS destino,\n    COUNT(*) AS total_corridas,\n    SUM(f.amt) AS receita_total,\n    AVG(f.amt) AS ticket_medio,\n    AVG(f.dst) AS distancia_media\nFROM dwh.fat_cor f\nJOIN dwh.dim_loc pickup ON f.srk_pck = pickup.srk_loc\nJOIN dwh.dim_loc drop ON f.srk_drp = drop.srk_loc\nWHERE f.flg_cmp = 'Yes'\nGROUP BY pickup.loc_nme, drop.loc_nme\nORDER BY receita_total DESC\nLIMIT 10;\n\"\"\", conn)\nconn.close()\n\nprint(\"\\n TOP 10 ROTAS MAIS RENTÁVEIS:\\n\")\ndf_top_rotas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63199c07",
   "metadata": {},
   "source": [
    "## 7. SUMÁRIO FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e319510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "                ETL SILVER → GOLD CONCLUÍDO! \n",
      "======================================================================\n",
      "\n",
      " RESUMO DA CARGA:\n",
      "----------------------------------------------------------------------\n",
      "Dimensão DateTime (dtt)...........................         124,452 registros\n",
      "Dimensão Customer (cst)...........................         147,580 registros\n",
      "Dimensão Location (loc)...........................             176 registros\n",
      "Dimensão Ride (rid)...............................         102,023 registros\n",
      " FATO CORRIDAS (fat_cor).........................         148,767 registros\n",
      "\n",
      "======================================================================\n",
      " Data Warehouse pronto para análises!\n",
      " Mnemônicos documentados em: docs/Mnemonicos_DWH.md\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*70)\nprint(\" \" * 15 + \" ETL SILVER → GOLD CONCLUÍDO! \")\nprint(\"=\"*70)\nprint(\"\\n RESUMO DA CARGA:\")\nprint(\"-\"*70)\n\nconn = get_connection()\ncur = conn.cursor()\ntabelas = [\n    ('dwh.dim_dtt', 'Dimensão DateTime (dtt)'),\n    ('dwh.dim_cst', 'Dimensão Customer (cst)'),\n    ('dwh.dim_loc', 'Dimensão Location (loc)'),\n    ('dwh.dim_rid', 'Dimensão Ride (rid)'),\n    ('dwh.fat_cor', ' FATO CORRIDAS (fat_cor)')\n]\n\nfor tabela, descricao in tabelas:\n    cur.execute(f\"SELECT COUNT(*) FROM {tabela};\")\n    count = cur.fetchone()[0]\n    print(f\"{descricao:.<50} {count:>15,} registros\")\n\ncur.close()\nconn.close()\nprint(\"\\n\" + \"=\"*70)\nprint(\" Data Warehouse pronto para análises!\")\nprint(\" Mnemônicos documentados em: docs/Mnemonicos_DWH.md\")\nprint(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}